# ğŸ´ Cutlery Classifier MVP (Enhanced Version)

> âœ¨ This is the enhanced version (V2) of the project that received the highest grade (VG). It includes proper Python packaging, ONNX export support, and improved testing while maintaining the original high-quality structure and documentation.

An image classification system for automated cutlery sorting, built with PyTorch and ResNet18.

## ğŸ†• V2 Improvements

1. **Proper Python Packaging**

   - Implemented `setup.py` and `pyproject.toml`
   - Correct package structure with `src/` layout
   - No PYTHONPATH dependencies
   - Easy installation with `pip install -e .`

2. **Enhanced Testing**

   - Complete pytest suite
   - End-to-end inference tests
   - Model architecture verification
   - Dataset validation

3. **Extended Functionality**
   - ONNX export support for edge deployment
   - Improved Grad-CAM visualization
   - Robust evaluation process
   - Detailed results reporting

## ğŸ“‹ Features

- Production-grade image classification pipeline
- Pre-trained ResNet18 with grayscale optimization
- Grad-CAM visualization support
- Comprehensive evaluation metrics
- Full test suite
- ONNX export capability

## ğŸ¯ Performance

| Metric    | Score |
| --------- | ----- |
| Accuracy  | 100%  |
| Precision | 100%  |
| Recall    | 100%  |
| F1-Score  | 100%  |

### Per-Class Performance

| Class | Precision | Recall | F1-Score | Support |
| ----- | --------- | ------ | -------- | ------- |
| Fork  | 100%      | 100%   | 100%     | 10      |
| Knife | 100%      | 100%   | 100%     | 10      |
| Spoon | 100%      | 100%   | 100%     | 10      |

## ğŸš€ Quick Start

### Installation

1. Clone the repository:

```bash
git clone https://github.com/olablom/cutlery-classifier-mvp.git
cd cutlery-classifier-mvp
```

2. Create and activate a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# OR
venv\Scripts\activate  # Windows
```

3. Install the package in development mode:

```bash
pip install -e .
```

### Training

Train the model with:

```bash
python scripts/train_pipeline.py --device cpu --config config/train_config.yaml
```

The best model will be saved to `models/checkpoints/type_detector_best.pth`.

### Inference

Run inference on a single image:

```bash
python scripts/run_inference.py --device cpu --image path/to/image.jpg
```

Add `--visualize` to generate Grad-CAM visualization:

```bash
python scripts/run_inference.py --device cpu --image path/to/image.jpg --visualize
```

### Evaluation

Run full test set evaluation:

```bash
python scripts/evaluate_on_test_set.py --device cpu
```

This will generate:

- `results/evaluation/confusion_matrix.png`
- `results/evaluation/metrics.json`

### Testing

Run the test suite:

```bash
pytest
```

## ğŸ“ Project Structure

```
FINAL_DELIVERY_V2/
â”œâ”€â”€ config/                 # Configuration files
â”‚   â””â”€â”€ train_config.yaml  # Training configuration
â”œâ”€â”€ data/                  # Dataset
â”‚   â””â”€â”€ simplified/        # Processed dataset
â”‚       â”œâ”€â”€ train/        # Training images
â”‚       â”œâ”€â”€ val/          # Validation images
â”‚       â””â”€â”€ test/         # Test images
â”œâ”€â”€ models/                # Model checkpoints
â”œâ”€â”€ results/              # Evaluation results
â”‚   â”œâ”€â”€ evaluation/      # Test set metrics
â”‚   â””â”€â”€ grad_cam/        # Grad-CAM visualizations
â”œâ”€â”€ scripts/              # Training and inference scripts
â”œâ”€â”€ src/                  # Source code
â”‚   â””â”€â”€ cutlery_classifier/
â”‚       â”œâ”€â”€ data/        # Data loading and transforms
â”‚       â”œâ”€â”€ inference/   # Inference pipeline
â”‚       â”œâ”€â”€ models/      # Model architecture
â”‚       â””â”€â”€ training/    # Training pipeline
â””â”€â”€ tests/               # Test suite
```

## ğŸ–¼ï¸ Example Results

### Grad-CAM Visualization

![Grad-CAM Example](results/grad_cam/gradcam_example.jpg)

### Confusion Matrix

![Confusion Matrix](results/evaluation/confusion_matrix.png)

## ğŸ“Š Model Architecture

- Base: ResNet18 (pretrained)
- Input: 320x320 grayscale
- Output: 3 classes (fork, knife, spoon)
- Parameters: 11.2M

## ğŸ› ï¸ Configuration

Key training parameters (from `config/train_config.yaml`):

- Learning rate: 0.001
- Batch size: 32
- Epochs: 30 (with early stopping)
- Optimizer: Adam
- Data augmentation: horizontal flip, rotation, blur

## ğŸ“ License

MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- PyTorch team for the excellent deep learning framework
- torchvision for the pre-trained ResNet18 model
- pytorch-grad-cam for visualization support
